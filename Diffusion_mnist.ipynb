{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bdcfdff6",
      "metadata": {
        "id": "bdcfdff6"
      },
      "source": [
        "# Diffusion Models for Generative AI\n",
        "\n",
        "Diffusion models are a class of generative models that create new data by reversing a process that gradually adds noise. They operate in two main phases:\n",
        "\n",
        "- **Forward Process (Diffusion):** Noise is incrementally added to the original data over $T$ steps, transforming it into pure noise.\n",
        "- **Model Training:** The model learns to predict the noise added at each step, enabling it to understand how to reverse the process.\n",
        "- **Backward Process (Denoising):** Starting from random noise, the model iteratively removes noise over $T$ steps, reconstructing a new, realistic sample.\n",
        "\n",
        "This approach allows diffusion models to generate new high-quality images from a image dataset by simulating the process of denoising."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc080f89",
      "metadata": {
        "id": "cc080f89"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3b58621e",
      "metadata": {
        "id": "3b58621e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "43e3c4e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43e3c4e1",
        "outputId": "dc3d04fb-68b8-43a0-966e-25beb4dd9208"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.88MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 129kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.03MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.22MB/s]\n"
          ]
        }
      ],
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "mnist_train = mnist.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_test = mnist.MNIST(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52ac6abf",
      "metadata": {
        "id": "52ac6abf"
      },
      "source": [
        "### Forward Process - Noising\n",
        "\n",
        "Let $x_0$ be the image at t=0, and $x_t$ be the data at timestep $t$ after adding noise. The forward process is defined as:\n",
        "\n",
        "$$x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1 - \\alpha_t} \\epsilon_t,\\quad \\epsilon_t \\sim N(0, 1)$$\n",
        "\n",
        "$$q(x_t | x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\quad \\beta_t \\mathbf{I}),\\quad t \\sim N(0, T)$$\n",
        "\n",
        "\n",
        "\n",
        "where $\\beta_t = 1 - \\alpha_t$ is the variance schedule for timestep $t$.\n",
        "\n",
        "The cumulative process from $x_0$ to $x_t$:\n",
        "\n",
        "$$\n",
        "q(x_t | x_0) = N(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, \\quad (1 - \\bar{\\alpha}_t) \\mathbf{I})\n",
        "$$\n",
        "\n",
        "where $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$.\n",
        "\n",
        "#### Algorithm\n",
        "\n",
        "1. **Initialize** $x_0$ as the original data sample.\n",
        "2. **For** $t = 1$ to $T$:\n",
        "    - Sample noise $\\epsilon_t \\sim N(0, \\mathbf{I})$\n",
        "    - Compute $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t$\n",
        "3. **Return** $x_T$ as the fully noised data.\n",
        "\n",
        "This process gradually transforms the original data into pure noise over $T$ steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7b20926f",
      "metadata": {
        "id": "7b20926f"
      },
      "outputs": [],
      "source": [
        "T = 30  # total timesteps\n",
        "a = 0.1 # noise proportion per step\n",
        "# kept constant for simplicity\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def Noising(images, T, a):\n",
        "    t = torch.randint(0, T, (images.size(0),), device=images.device).float()\n",
        "    noise = torch.randn_like(images)\n",
        "    a_t = a**t\n",
        "\n",
        "    noisy_images = (a_t.view(-1, 1, 1, 1))**0.5 * images + (1 - a_t).view(-1, 1, 1, 1)**0.5 * noise\n",
        "\n",
        "    return noisy_images, noise , t  # -> this should also be output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c41ae8b8",
      "metadata": {
        "id": "c41ae8b8"
      },
      "source": [
        "### Deep Learning Model ###\n",
        "A simple CNN based model to run over-the image reduse the dimensions and atlast give an output $\\epsilon_t$.\\\n",
        "$\\epsilon_t$ is a prediction of the amount of error introduced. And, the model learns to predict $\\epsilon_t$ better and better.\n",
        "\n",
        "## U-NET Model Architecture\n",
        "\n",
        "This model is inspired by the **U-Net architecture**, with additional **time embeddings** to condition the network on the diffusion step `t`.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Components\n",
        "- **DoubleConv**: Two consecutive `Conv2d + ReLU` layers, used as the basic building block.  \n",
        "- **Down (Encoder path)**: `MaxPool2d` for downsampling, followed by `DoubleConv`. Captures hierarchical features.  \n",
        "- **Up (Decoder path)**: `Upsample` to increase spatial resolution, concatenates encoder features via skip connections, then applies `DoubleConv` to preserve fine details.  \n",
        "- **OutConv**: Final `1×1 convolution` to reduce channels to match output image size.  \n",
        "- **Time Embedding (`time_mlp`)**: Small MLP that processes the diffusion step `t` into an embedding, conditioning the model on the current timestep.\n",
        "\n",
        "---\n",
        "\n",
        "### Forward Pass\n",
        "1. Input image `x` goes through the encoder:  \n",
        "   `inc → down1 → down2 → down3 → down4`  \n",
        "2. Decoder reconstructs the image with skip connections:  \n",
        "   `up1(x5, x4) → up2(x, x3) → up3(x, x2) → up4(x, x1)`  \n",
        "3. Final output produced with `OutConv`.  \n",
        "4. Diffusion step `t` is embedded via `time_mlp` and conditions the model during training.\n",
        "\n",
        "---\n",
        "\n",
        "### Model Usage\n",
        "- **Input**: noisy image `x` and diffusion step `t`  \n",
        "- **Output**: predicted denoised image (same shape as `x`)  \n",
        "\n",
        "This design makes the model suitable for **denoising diffusion probabilistic models (DDPMs)** and other generative tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a7cb3b0b",
      "metadata": {
        "id": "a7cb3b0b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Diffusion(nn.Module):\n",
        "    def __init__(self, img_channels=1, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.inc = DoubleConv(img_channels, hidden_dim)\n",
        "        self.down1 = Down(hidden_dim, hidden_dim * 2)\n",
        "        self.down2 = Down(hidden_dim * 2, hidden_dim * 4)\n",
        "        self.down3 = Down(hidden_dim * 4, hidden_dim * 8)\n",
        "        self.down4 = Down(hidden_dim * 8, hidden_dim * 8)\n",
        "        self.up1 = Up(hidden_dim * 16, hidden_dim * 4)\n",
        "        self.up2 = Up(hidden_dim * 8, hidden_dim * 2)\n",
        "        self.up3 = Up(hidden_dim * 4, hidden_dim)\n",
        "        self.up4 = Up(hidden_dim * 2, hidden_dim)\n",
        "        self.outc = OutConv(hidden_dim, img_channels)\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(1, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t = t.view(-1, 1).float() / 1000.0  # scale down\n",
        "        t_emb = self.time_mlp(t)[:, :, None, None]\n",
        "\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        out = self.outc(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc83b981",
      "metadata": {
        "id": "dc83b981"
      },
      "source": [
        "### Model Training\n",
        "- Design a loss function to train for $\\epsilon_t$.\n",
        "- For simplicity MSELoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "07b692e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "07b692e3",
        "outputId": "c881b3dd-13a1-4a6b-aad5-6959cd462bd4"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-556051840.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1571\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGrayscaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \"\"\"\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb_to_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_output_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mrgb_to_grayscale\u001b[0;34m(img, num_output_channels)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_output_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0mchannel\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \"\"\"\n\u001b[0;32m-> 1286\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_to_grayscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mis_tracing\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1325\u001b[0m     tracing of code with ``torch.jit.trace``) and ``False`` otherwise.\n\u001b[1;32m   1326\u001b[0m     \"\"\"\n\u001b[0;32m-> 1327\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mis_scripting\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     r\"\"\"\n\u001b[1;32m    105\u001b[0m     \u001b[0mFunction\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompilation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_epochs = 20\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = Diffusion().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "T = 30\n",
        "a = 0.9 # decay factor\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, _ in train_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        noisy_images, noise, t = Noising(images, T, a)\n",
        "\n",
        "        noise_pred = model(noisy_images, t)\n",
        "\n",
        "        loss = criterion(noise_pred, noise)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), 'diffusion_unet_model.pth')\n",
        "print(\"Model weights saved to diffusion_unet_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deef38ce",
      "metadata": {
        "id": "deef38ce"
      },
      "source": [
        "#Denoising\n",
        "\n",
        "The full denoising update at each step is:\n",
        "\n",
        "$$\n",
        "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\alpha_t$: noise schedule at step $t$\n",
        "- $\\bar{\\alpha}_t$: cumulative product of $\\alpha_t$ up to $t$\n",
        "- $\\epsilon_\\theta(x_t, t)$: predicted noise by the model\n",
        "- $\\sigma_t$: standard deviation for added noise\n",
        "- $z \\sim N(0, I)$: random noise\n",
        "\n",
        "This update produces higher fidelity samples by accounting for the variance schedule.\n",
        "$$\n",
        "q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\n",
        "= N(\\mathbf{x}_{t-1}; \\mu_q(\\mathbf{x}_t, \\mathbf{x}_0), \\sigma_q^2(t)\\mathbf{I}).\n",
        "\\tag{14}\n",
        "$$\n",
        "\n",
        "Where,\n",
        "$$\n",
        "\\sigma_q^2(t) = \\frac{(1 - \\alpha_t)(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}.\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mu_q(\\mathbf{x}_t, \\mathbf{x}_0) = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})\\mathbf{x}_t + \\sqrt{\\bar{\\alpha}_{t-1}}(1 - \\alpha_t)\\mathbf{x}_0}{1 - \\bar{\\alpha}_t}.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3bc269e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "3bc269e1",
        "outputId": "59f7c30c-7f9e-4486-f27d-d6e1f1f238ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model weights loaded successfully.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 84x84 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@torch.no_grad()\n",
        "def Denoising(model, img_size=(1, 28, 28), T=30, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    betas = torch.linspace(1e-4, 0.02, T, device=device)\n",
        "    alphas = 1.0 - betas\n",
        "    alpha_bar = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "    # pure noise\n",
        "    x_t = torch.randn(1, *img_size, device=device)\n",
        "\n",
        "    for t in reversed(range(T)):\n",
        "        t_tensor = torch.tensor([T], device=device, dtype=torch.long)\n",
        "\n",
        "        eps_theta = model(x_t, t_tensor)\n",
        "\n",
        "        alpha_t = alphas[t]\n",
        "        alpha_bar_t = alpha_bar[t]\n",
        "        beta_t = betas[t]\n",
        "\n",
        "        mean = (1.0 / torch.sqrt(alpha_t)) * (\n",
        "            x_t - (beta_t / torch.sqrt(1 - alpha_bar_t)) * eps_theta\n",
        "        )\n",
        "\n",
        "        if t > 0:\n",
        "            # variance term σ_t z\n",
        "            alpha_bar_prev = alpha_bar[t-1] if t > 0 else torch.tensor(1.0, device=device)\n",
        "            posterior_var = beta_t * (1 - alpha_bar_prev) / (1 - alpha_bar_t)\n",
        "            sigma_t = torch.sqrt(posterior_var)\n",
        "            noise = torch.randn_like(x_t)\n",
        "            x_t = mean + sigma_t * noise\n",
        "        else:\n",
        "            # at t=0, just take the mean\n",
        "            x_t = mean\n",
        "\n",
        "    return x_t\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = Diffusion().to(device)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('diffusion_unet_model.pth', map_location=device))\n",
        "        print(\"Model weights loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Model weights file not found. Please train the model first.\")\n",
        "        exit()\n",
        "\n",
        "    gen_image = Denoising(model, img_size=(1, 28, 28), T=1000, device= device)\n",
        "\n",
        "    # denormalize the image for display\n",
        "    gen_image = gen_image * 0.5 + 0.5\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(28/100, 28/100), dpi=300)\n",
        "    ax.imshow(gen_image.squeeze().cpu(), cmap=\"gray\")\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
